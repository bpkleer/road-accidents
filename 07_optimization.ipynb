{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH29u_arvwLT"
   },
   "source": [
    "# Optimization\n",
    "\n",
    "In the following, I ran optimization with `GridSearchCV` for the two best models: \n",
    "\n",
    "- `Random Forest` (only all features)\n",
    "- `XGBoost`\n",
    "- `CatBoost`\n",
    "\n",
    "Since the full data set and Select-K-Best data set performed closely, I run the grid search for both data sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOa7cqAFwMaB"
   },
   "source": [
    "## Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "pZD1icEsszqL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error, r2_score\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "\n",
    "# for wrapper around XGBoostRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "# Explainer Dashboard\n",
    "from explainerdashboard import RegressionExplainer, ExplainerDashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User functions\n",
    "# Prepare markdown table\n",
    "def scores_to_markdown(scores):\n",
    "    # Create table header\n",
    "    header = '| Model | MAE | MSE | RMSE | R2 |\\n'\n",
    "    header += '|-------|-----|-----|------|-----|\\n'\n",
    "    \n",
    "    # Create table rows\n",
    "    table_rows = ''\n",
    "    for model_name, metrics in scores.items():\n",
    "        table_rows += f'| {model_name} | {metrics['MAE']:.4f} | {metrics['MSE']:.4f} | {metrics['RMSE']:.4f} | {metrics['R2']:.4f} |\\n'\n",
    "    \n",
    "    return header + table_rows\n",
    "\n",
    "# Fixed issues with __sklearn_tags__\n",
    "class SklearnXGBRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs  # Store the parameters for later use\n",
    "        self.model = XGBRegressor(**self.kwargs)  # Pass them to XGBRegressor\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return self.kwargs  # Return stored parameters for compatibility\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        self.kwargs.update(params)  # Update the parameters\n",
    "        self.model = XGBRegressor(**self.kwargs)  # Recreate the model with new parameters\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3erKh-8wOpu"
   },
   "source": [
    "# No selection data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vv5Hg_nzOgOp",
    "outputId": "e43d01ab-47e8-4211-9bc6-a236432e1a28"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/processed/train_eng.csv')\n",
    "\n",
    "y_train = train['severity_score']\n",
    "X_train = train.drop(columns='severity_score', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'XGBoost': SklearnXGBRegressor(random_state=42),\n",
    "    'CatBoost': CatBoostRegressor(verbose=200)\n",
    "}\n",
    "\n",
    "names = ['Mean Prediction', 'Random Forest', 'XGBoost', 'CatBoost']\n",
    "\n",
    "# Dictionary to store scores\n",
    "metrics = ['MAE', 'MSE', 'RMSE', 'R2']\n",
    "scores = {model_name: {metric: [] for metric in metrics} for model_name in names}\n",
    "\n",
    "scoring = {\n",
    "    'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "    'MSE': make_scorer(mean_squared_error, greater_is_better=False),\n",
    "    'R2': 'r2'\n",
    "}\n",
    "\n",
    "# Mean prediction as baseline model\n",
    "mean_value = np.mean(y_train)\n",
    "mean_predictions = np.full_like(y_train, mean_value)\n",
    "\n",
    "# Store baseline model metrics\n",
    "scores['Mean Prediction']['MAE'] = mean_absolute_error(y_train, mean_predictions)\n",
    "scores['Mean Prediction']['MSE'] = mean_squared_error(y_train, mean_predictions)\n",
    "scores['Mean Prediction']['RMSE'] = np.sqrt(mean_squared_error(y_train, mean_predictions))\n",
    "scores['Mean Prediction']['R2'] = r2_score(y_train, mean_predictions)\n",
    "\n",
    "# Hyperparameter grids for GridSearchCV\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 6, 10],\n",
    "        'subsample': [0.7, 0.8, 1.0]\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'iterations': [500, 1000],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'depth': [6, 10, 12],\n",
    "        'l2_leaf_reg': [1, 3, 5],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform Grid Search and Cross-Validation for each model\n",
    "for model_name, model in models.items():\n",
    "    print(f'\\nPerforming Grid Search for {model_name}...')\n",
    "\n",
    "    # Set the parameter grid for the model\n",
    "    param_grid = param_grids[model_name]\n",
    "\n",
    "    # Grid Search with Cross-Validation\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring['R2'], cv=5, n_jobs=-1, verbose=1, refit=True)\n",
    "\n",
    "    # Fit GridSearchCV to the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best estimator after grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Save the best model using joblib\n",
    "    joblib.dump(best_model, f'output/grid_{model_name}_best_model.pkl')\n",
    "    print(f'Best model for {model_name} saved as grid_{model_name}_best_model.pkl')\n",
    "\n",
    "    # Get the best score and parameters\n",
    "    best_score = grid_search.best_score_\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    print(f\"Best R2 score for {model_name}: {best_score:.4f}\")\n",
    "    print(f\"Best parameters for {model_name}: {best_params}\")\n",
    "\n",
    "    # Get the cross-validation results for the best model\n",
    "    cv_results = cross_validate(best_model, X_train, y_train, cv=5, scoring=scoring, return_train_score=False)\n",
    "    \n",
    "    # Store the scores for this model\n",
    "    scores[model_name]['MAE'] = -np.mean(cv_results['test_MAE'])  # Negate for negative MAE\n",
    "    scores[model_name]['MSE'] = -np.mean(cv_results['test_MSE'])  # Negate for negative MSE\n",
    "    scores[model_name]['RMSE'] = np.sqrt(scores[model_name]['MSE'])\n",
    "    scores[model_name]['R2'] = np.mean(cv_results['test_R2'])\n",
    "\n",
    "# Print scores\n",
    "for model_name, metrics in scores.items():\n",
    "    print(f'\\n{model_name}:')\n",
    "    for metric, value in metrics.items():\n",
    "        print(f'{metric}: {value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the markdown table and save to file\n",
    "markdown_table = scores_to_markdown(scores)\n",
    "with open('output/gridsearch_model_scores.md', 'w') as f:\n",
    "    f.write(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting\n",
    "plot_data = {metric: [scores[model][metric] for model in names] for metric in metrics}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "for metric, values in plot_data.items():\n",
    "    plt.plot(names, values, label=metric)\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Regression Model Performance Metrics')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('output/gridsearch-all-performance.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/processed/test_eng.csv')\n",
    "y_test = test['severity_score']\n",
    "X_test = test.drop(columns='severity_score', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on whole train data set\n",
    "final = SklearnXGBRegressor(learning_rate=0.1, max_depth=6,n_estimators=200, subsample=1.0)\n",
    "\n",
    "final.fit(X_train, y_train)\n",
    "y_pred = final.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R2 Score:\", r2)\n",
    "\n",
    "header = '| Model | MAE | MSE | RMSE | R2 |\\n'\n",
    "header += '|-------|-----|-----|------|-----|\\n'\n",
    "header += f'| XGBoost | {mae} | {mse} | {rmse} | {r2}| \\n' \n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=y_pred, y=y_test)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Explainer Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(learning_rate=0.1, max_depth=6,n_estimators=200, subsample=1.0).fit(X_train, y_train)\n",
    "\n",
    "explainer = RegressionExplainer(model, X_test, y_test)\n",
    "\n",
    "ExplainerDashboard(explainer).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Selection data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/processed/train_ksel.csv', index_col=0)\n",
    "\n",
    "y_train = train['severity_score']\n",
    "X_train = train.drop(columns='severity_score', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'XGBoost': SklearnXGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    'CatBoost': CatBoostRegressor(iterations=1000, learning_rate=0.05, depth=10, verbose=200)\n",
    "}\n",
    "\n",
    "names = ['Mean Prediction', 'XGBoost', 'CatBoost']\n",
    "\n",
    "# Dictionary to store scores\n",
    "metrics = ['MAE', 'MSE', 'RMSE', 'R2']\n",
    "scores_ksel = {model_name: {metric: [] for metric in metrics} for model_name in names}\n",
    "\n",
    "scoring = {\n",
    "    'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "    'MSE': make_scorer(mean_squared_error, greater_is_better=False),\n",
    "    'R2': 'r2'\n",
    "}\n",
    "\n",
    "# Mean prediction as baseline model\n",
    "mean_value = np.mean(y_train)\n",
    "mean_predictions = np.full_like(y_train, mean_value)\n",
    "\n",
    "# Store baseline model metrics\n",
    "scores_ksel['Mean Prediction']['MAE'] = mean_absolute_error(y_train, mean_predictions)\n",
    "scores_ksel['Mean Prediction']['MSE'] = mean_squared_error(y_train, mean_predictions)\n",
    "scores_ksel['Mean Prediction']['RMSE'] = np.sqrt(mean_squared_error(y_train, mean_predictions))\n",
    "scores_ksel['Mean Prediction']['R2'] = r2_score(y_train, mean_predictions)\n",
    "\n",
    "# Hyperparameter grids for GridSearchCV\n",
    "param_grids = {\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 6, 10],\n",
    "        'subsample': [0.7, 0.8, 1.0]\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'iterations': [500, 1000],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'depth': [6, 10, 12],\n",
    "        'l2_leaf_reg': [1, 3, 5],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform Grid Search and Cross-Validation for each model\n",
    "for model_name, model in models.items():\n",
    "    print(f'\\nPerforming Grid Search for {model_name}...')\n",
    "\n",
    "    # Set the parameter grid for the model\n",
    "    param_grid = param_grids[model_name]\n",
    "\n",
    "    # Grid Search with Cross-Validation\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring['R2'], cv=5, n_jobs=-1, verbose=1, refit=True)\n",
    "\n",
    "    # Fit GridSearchCV to the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best estimator after grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Save the best model using joblib\n",
    "    joblib.dump(best_model, f'output/grid_{model_name}_best_model_ksel.pkl')\n",
    "    print(f'Best model for {model_name} saved as grid_{model_name}_best_model_ksel.pkl')\n",
    "\n",
    "    # Get the best score and parameters\n",
    "    best_score = grid_search.best_score_\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    print(f\"Best R2 score for {model_name}: {best_score:.4f}\")\n",
    "    print(f\"Best parameters for {model_name}: {best_params}\")\n",
    "\n",
    "    # Get the cross-validation results for the best model\n",
    "    cv_results = cross_validate(best_model, X_train, y_train, cv=5, scoring=scoring, return_train_score=False)\n",
    "    \n",
    "    # Store the scores for this model\n",
    "    scores_ksel[model_name]['MAE'] = -np.mean(cv_results['test_MAE'])  # Negate for negative MAE\n",
    "    scores_ksel[model_name]['MSE'] = -np.mean(cv_results['test_MSE'])  # Negate for negative MSE\n",
    "    scores_ksel[model_name]['RMSE'] = np.sqrt(scores[model_name]['MSE'])\n",
    "    scores_ksel[model_name]['R2'] = np.mean(cv_results['test_R2'])\n",
    "\n",
    "# Print scores\n",
    "for model_name, metrics in scores_ksel.items():\n",
    "    print(f'\\n{model_name}:')\n",
    "    for metric, value in metrics.items():\n",
    "        print(f'{metric}: {value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the markdown table and save to file\n",
    "markdown_table = scores_to_markdown(scores_ksel)\n",
    "with open('output/gridsearch_model_scores_ksel.md', 'w') as f:\n",
    "    f.write(markdown_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting\n",
    "plot_data = {metric: [scores_ksel[model][metric] for model in names] for metric in metrics}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "for metric, values in plot_data.items():\n",
    "    plt.plot(names, values, label=metric)\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Regression Model Performance Metrics')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('output/gridsearch-ksel-performance.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/processed/test_ksel.csv', index_col=0)\n",
    "y_test = test['severity_score']\n",
    "X_test = test.drop(columns='severity_score', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on whole train data set\n",
    "final = CatBoostRegressor(learning_rate=0.1, depth=6, n_estimators=1000, l2_leaf_reg=5)\n",
    "\n",
    "final.fit(X_train, y_train)\n",
    "y_pred = final.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"R2 Score:\", r2)\n",
    "\n",
    "header = '| Model | MAE | MSE | RMSE | R2 |\\n'\n",
    "header += '|-------|-----|-----|------|-----|\\n'\n",
    "header += f'| CatBoost | {mae} | {mse} | {rmse} | {r2}| \\n' \n",
    "\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=y_pred, y=y_test)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Explainer Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostRegressor(learning_rate=0.1, depth=6, n_estimators=1000, l2_leaf_reg=5).fit(X_train, y_train)\n",
    "\n",
    "explainer = RegressionExplainer(model, X_test, y_test)\n",
    "\n",
    "ExplainerDashboard(explainer).run()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
